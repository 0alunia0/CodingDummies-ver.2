{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba7dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/michal/miniconda3/envs/nasa/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image'],\n",
      "    num_rows: 15050\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "images_path = \"/home/michal/projects/nasa/data/full_train/full/img\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load images into a Hugging Face dataset\n",
    "dataset = load_dataset(\"imagefolder\", data_dir=images_path)\n",
    "dataset = dataset[\"train\"]  # access the split\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b09059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "import torch\n",
    "\n",
    "model_name = \"google/vit-base-patch16-224\"\n",
    "processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b7e61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/15050 [00:00<?, ? examples/s]/home/michal/miniconda3/envs/nasa/lib/python3.13/site-packages/transformers/image_processing_utils.py:51: UserWarning: The following named arguments are not valid for `ViTImageProcessor.preprocess` and were ignored: 'padding'\n",
      "  return self.preprocess(images, **kwargs)\n",
      "Map: 100%|██████████| 15050/15050 [12:14<00:00, 20.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def extract_vit_embeddings(batch):\n",
    "    # Convert to RGB\n",
    "    images = [img.convert(\"RGB\") for img in batch[\"image\"]]\n",
    "    # Preprocess for ViT\n",
    "    inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # CLS token (global representation)\n",
    "        embeddings = outputs.pooler_output.cpu()\n",
    "\n",
    "    batch[\"vit_embedding\"] = embeddings\n",
    "    return batch\n",
    "\n",
    "# Apply with batching\n",
    "dataset = dataset.map(extract_vit_embeddings, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4af4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'vit_embedding'],\n",
       "    num_rows: 15050\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ced6deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dataset[\"vit_embedding\"], \"vit_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1772f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 15050/15050 [00:00<00:00, 26703.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"vit_dataset.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdf296db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "287779f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/michal/projects/nasa/data/full_train/full/data/all_data.txt\",  index_col=None, sep=r\"\\s+\", header=None, names=[\"rel_x\", \"rel_y\", \"rel_z\", \"tar_x\", \"tar_y\", \"tar_z\",\"tar_w\", \"chas_x\", \"chas_y\", \"chas_z\", \"chas_w\", \"time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f41040b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df[[\"rel_x\", \"rel_y\", \"rel_z\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5174ee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15050, 771)\n"
     ]
    }
   ],
   "source": [
    "X_full = np.concatenate([dataset[\"vit_embedding\"], numeric_features], axis=1, dtype=np.float32)\n",
    "print(X_full.shape)  # [N, 1003]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "edab912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y  = np.array(df[[\"tar_x\", \"tar_y\", \"tar_z\",\"tar_w\", \"chas_x\", \"chas_y\", \"chas_z\", \"chas_w\"]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1 — train vs temp (val+test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X_full, y, test_size=0.3, random_state=42, shuffle=True)\n",
    "\n",
    "# Step 2 — split temp into val and test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19265311",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10535, 8])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(y_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aade8300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/200 | Train Loss: 0.174482 | Val Loss: 0.126907\n",
      "Epoch 02/200 | Train Loss: 0.137862 | Val Loss: 0.128172\n",
      "Epoch 03/200 | Train Loss: 0.131304 | Val Loss: 0.125753\n",
      "Epoch 04/200 | Train Loss: 0.128320 | Val Loss: 0.125107\n",
      "Epoch 05/200 | Train Loss: 0.127331 | Val Loss: 0.125323\n",
      "Epoch 06/200 | Train Loss: 0.125797 | Val Loss: 0.124828\n",
      "Epoch 07/200 | Train Loss: 0.124849 | Val Loss: 0.123447\n",
      "Epoch 08/200 | Train Loss: 0.123394 | Val Loss: 0.122663\n",
      "Epoch 09/200 | Train Loss: 0.122313 | Val Loss: 0.121954\n",
      "Epoch 10/200 | Train Loss: 0.120736 | Val Loss: 0.120779\n",
      "Epoch 11/200 | Train Loss: 0.118817 | Val Loss: 0.119922\n",
      "Epoch 12/200 | Train Loss: 0.117455 | Val Loss: 0.118945\n",
      "Epoch 13/200 | Train Loss: 0.116552 | Val Loss: 0.121341\n",
      "Epoch 14/200 | Train Loss: 0.115719 | Val Loss: 0.116827\n",
      "Epoch 15/200 | Train Loss: 0.114475 | Val Loss: 0.117648\n",
      "Epoch 16/200 | Train Loss: 0.112403 | Val Loss: 0.115500\n",
      "Epoch 17/200 | Train Loss: 0.111700 | Val Loss: 0.116747\n",
      "Epoch 18/200 | Train Loss: 0.110615 | Val Loss: 0.114593\n",
      "Epoch 19/200 | Train Loss: 0.109011 | Val Loss: 0.113880\n",
      "Epoch 20/200 | Train Loss: 0.108518 | Val Loss: 0.115230\n",
      "Epoch 21/200 | Train Loss: 0.107036 | Val Loss: 0.112198\n",
      "Epoch 22/200 | Train Loss: 0.105371 | Val Loss: 0.112973\n",
      "Epoch 23/200 | Train Loss: 0.104611 | Val Loss: 0.112378\n",
      "Epoch 24/200 | Train Loss: 0.103529 | Val Loss: 0.112864\n",
      "Epoch 25/200 | Train Loss: 0.102169 | Val Loss: 0.109425\n",
      "Epoch 26/200 | Train Loss: 0.101564 | Val Loss: 0.110079\n",
      "Epoch 27/200 | Train Loss: 0.099332 | Val Loss: 0.110476\n",
      "Epoch 28/200 | Train Loss: 0.098838 | Val Loss: 0.107559\n",
      "Epoch 29/200 | Train Loss: 0.097972 | Val Loss: 0.107222\n",
      "Epoch 30/200 | Train Loss: 0.097158 | Val Loss: 0.110641\n",
      "Epoch 31/200 | Train Loss: 0.096306 | Val Loss: 0.107633\n",
      "Epoch 32/200 | Train Loss: 0.095494 | Val Loss: 0.105769\n",
      "Epoch 33/200 | Train Loss: 0.094183 | Val Loss: 0.106538\n",
      "Epoch 34/200 | Train Loss: 0.093372 | Val Loss: 0.106012\n",
      "Epoch 35/200 | Train Loss: 0.092672 | Val Loss: 0.105437\n",
      "Epoch 36/200 | Train Loss: 0.091324 | Val Loss: 0.104452\n",
      "Epoch 37/200 | Train Loss: 0.091652 | Val Loss: 0.105718\n",
      "Epoch 38/200 | Train Loss: 0.090614 | Val Loss: 0.104304\n",
      "Epoch 39/200 | Train Loss: 0.089487 | Val Loss: 0.104120\n",
      "Epoch 40/200 | Train Loss: 0.089218 | Val Loss: 0.104435\n",
      "Epoch 41/200 | Train Loss: 0.088659 | Val Loss: 0.102908\n",
      "Epoch 42/200 | Train Loss: 0.087139 | Val Loss: 0.104280\n",
      "Epoch 43/200 | Train Loss: 0.087445 | Val Loss: 0.103034\n",
      "Epoch 44/200 | Train Loss: 0.087319 | Val Loss: 0.104356\n",
      "Epoch 45/200 | Train Loss: 0.086231 | Val Loss: 0.103393\n",
      "Epoch 46/200 | Train Loss: 0.085870 | Val Loss: 0.102715\n",
      "Epoch 47/200 | Train Loss: 0.085183 | Val Loss: 0.103890\n",
      "Epoch 48/200 | Train Loss: 0.085031 | Val Loss: 0.103577\n",
      "Epoch 49/200 | Train Loss: 0.084238 | Val Loss: 0.104325\n",
      "Epoch 50/200 | Train Loss: 0.083756 | Val Loss: 0.103210\n",
      "Epoch 51/200 | Train Loss: 0.082989 | Val Loss: 0.106387\n",
      "Epoch 52/200 | Train Loss: 0.082390 | Val Loss: 0.102504\n",
      "Epoch 53/200 | Train Loss: 0.082406 | Val Loss: 0.105947\n",
      "Epoch 54/200 | Train Loss: 0.082426 | Val Loss: 0.104346\n",
      "Epoch 55/200 | Train Loss: 0.081120 | Val Loss: 0.104003\n",
      "Epoch 56/200 | Train Loss: 0.081253 | Val Loss: 0.106108\n",
      "Epoch 57/200 | Train Loss: 0.081246 | Val Loss: 0.102600\n",
      "Epoch 58/200 | Train Loss: 0.080144 | Val Loss: 0.104274\n",
      "Epoch 59/200 | Train Loss: 0.080462 | Val Loss: 0.101563\n",
      "Epoch 60/200 | Train Loss: 0.079580 | Val Loss: 0.106678\n",
      "Epoch 61/200 | Train Loss: 0.079594 | Val Loss: 0.105516\n",
      "Epoch 62/200 | Train Loss: 0.079338 | Val Loss: 0.103365\n",
      "Epoch 63/200 | Train Loss: 0.078578 | Val Loss: 0.103325\n",
      "Epoch 64/200 | Train Loss: 0.078166 | Val Loss: 0.106081\n",
      "Epoch 65/200 | Train Loss: 0.078763 | Val Loss: 0.104563\n",
      "Epoch 66/200 | Train Loss: 0.077835 | Val Loss: 0.106334\n",
      "Epoch 67/200 | Train Loss: 0.077252 | Val Loss: 0.107531\n",
      "Epoch 68/200 | Train Loss: 0.076824 | Val Loss: 0.105875\n",
      "Epoch 69/200 | Train Loss: 0.077012 | Val Loss: 0.105883\n",
      "Epoch 70/200 | Train Loss: 0.076243 | Val Loss: 0.105350\n",
      "Epoch 71/200 | Train Loss: 0.076137 | Val Loss: 0.105972\n",
      "Epoch 72/200 | Train Loss: 0.075641 | Val Loss: 0.106699\n",
      "Epoch 73/200 | Train Loss: 0.075708 | Val Loss: 0.104482\n",
      "Epoch 74/200 | Train Loss: 0.075494 | Val Loss: 0.107435\n",
      "Epoch 75/200 | Train Loss: 0.075129 | Val Loss: 0.105883\n",
      "Epoch 76/200 | Train Loss: 0.074858 | Val Loss: 0.107944\n",
      "Epoch 77/200 | Train Loss: 0.074907 | Val Loss: 0.108641\n",
      "Epoch 78/200 | Train Loss: 0.074695 | Val Loss: 0.108417\n",
      "Epoch 79/200 | Train Loss: 0.074073 | Val Loss: 0.108119\n",
      "Epoch 80/200 | Train Loss: 0.074049 | Val Loss: 0.108399\n",
      "Epoch 81/200 | Train Loss: 0.074198 | Val Loss: 0.108624\n",
      "Epoch 82/200 | Train Loss: 0.073127 | Val Loss: 0.109199\n",
      "Epoch 83/200 | Train Loss: 0.074053 | Val Loss: 0.105017\n",
      "Epoch 84/200 | Train Loss: 0.072660 | Val Loss: 0.106904\n",
      "Epoch 85/200 | Train Loss: 0.072629 | Val Loss: 0.108362\n",
      "Epoch 86/200 | Train Loss: 0.072628 | Val Loss: 0.110644\n",
      "Epoch 87/200 | Train Loss: 0.072706 | Val Loss: 0.108637\n",
      "Epoch 88/200 | Train Loss: 0.072480 | Val Loss: 0.107572\n",
      "Epoch 89/200 | Train Loss: 0.072288 | Val Loss: 0.112037\n",
      "Epoch 90/200 | Train Loss: 0.071383 | Val Loss: 0.109864\n",
      "Epoch 91/200 | Train Loss: 0.071579 | Val Loss: 0.108022\n",
      "Epoch 92/200 | Train Loss: 0.071057 | Val Loss: 0.107889\n",
      "Epoch 93/200 | Train Loss: 0.071191 | Val Loss: 0.109897\n",
      "Epoch 94/200 | Train Loss: 0.071315 | Val Loss: 0.108034\n",
      "Epoch 95/200 | Train Loss: 0.070309 | Val Loss: 0.111584\n",
      "Epoch 96/200 | Train Loss: 0.070907 | Val Loss: 0.109473\n",
      "Epoch 97/200 | Train Loss: 0.070016 | Val Loss: 0.108261\n",
      "Epoch 98/200 | Train Loss: 0.069801 | Val Loss: 0.109743\n",
      "Epoch 99/200 | Train Loss: 0.070401 | Val Loss: 0.109664\n",
      "Epoch 100/200 | Train Loss: 0.069771 | Val Loss: 0.110486\n",
      "Epoch 101/200 | Train Loss: 0.069573 | Val Loss: 0.113332\n",
      "Epoch 102/200 | Train Loss: 0.069297 | Val Loss: 0.109211\n",
      "Epoch 103/200 | Train Loss: 0.069595 | Val Loss: 0.109616\n",
      "Epoch 104/200 | Train Loss: 0.068841 | Val Loss: 0.110988\n",
      "Epoch 105/200 | Train Loss: 0.068856 | Val Loss: 0.109847\n",
      "Epoch 106/200 | Train Loss: 0.068704 | Val Loss: 0.110445\n",
      "Epoch 107/200 | Train Loss: 0.069069 | Val Loss: 0.112250\n",
      "Epoch 108/200 | Train Loss: 0.068323 | Val Loss: 0.108923\n",
      "Epoch 109/200 | Train Loss: 0.067840 | Val Loss: 0.115719\n",
      "Epoch 110/200 | Train Loss: 0.068498 | Val Loss: 0.114003\n",
      "Epoch 111/200 | Train Loss: 0.067887 | Val Loss: 0.112841\n",
      "Epoch 112/200 | Train Loss: 0.067704 | Val Loss: 0.112694\n",
      "Epoch 113/200 | Train Loss: 0.068007 | Val Loss: 0.112726\n",
      "Epoch 114/200 | Train Loss: 0.067107 | Val Loss: 0.111941\n",
      "Epoch 115/200 | Train Loss: 0.067302 | Val Loss: 0.111673\n",
      "Epoch 116/200 | Train Loss: 0.066947 | Val Loss: 0.114133\n",
      "Epoch 117/200 | Train Loss: 0.066571 | Val Loss: 0.114394\n",
      "Epoch 118/200 | Train Loss: 0.066759 | Val Loss: 0.114005\n",
      "Epoch 119/200 | Train Loss: 0.066498 | Val Loss: 0.113553\n",
      "Epoch 120/200 | Train Loss: 0.065938 | Val Loss: 0.113274\n",
      "Epoch 121/200 | Train Loss: 0.066284 | Val Loss: 0.112848\n",
      "Epoch 122/200 | Train Loss: 0.066365 | Val Loss: 0.115477\n",
      "Epoch 123/200 | Train Loss: 0.066332 | Val Loss: 0.115472\n",
      "Epoch 124/200 | Train Loss: 0.065831 | Val Loss: 0.113313\n",
      "Epoch 125/200 | Train Loss: 0.065215 | Val Loss: 0.115315\n",
      "Epoch 126/200 | Train Loss: 0.065388 | Val Loss: 0.116378\n",
      "Epoch 127/200 | Train Loss: 0.065339 | Val Loss: 0.115290\n",
      "Epoch 128/200 | Train Loss: 0.065654 | Val Loss: 0.115542\n",
      "Epoch 129/200 | Train Loss: 0.064763 | Val Loss: 0.113880\n",
      "Epoch 130/200 | Train Loss: 0.065731 | Val Loss: 0.115276\n",
      "Epoch 131/200 | Train Loss: 0.064827 | Val Loss: 0.114497\n",
      "Epoch 132/200 | Train Loss: 0.064912 | Val Loss: 0.117471\n",
      "Epoch 133/200 | Train Loss: 0.064826 | Val Loss: 0.116606\n",
      "Epoch 134/200 | Train Loss: 0.064350 | Val Loss: 0.116187\n",
      "Epoch 135/200 | Train Loss: 0.064568 | Val Loss: 0.117075\n",
      "Epoch 136/200 | Train Loss: 0.063809 | Val Loss: 0.116831\n",
      "Epoch 137/200 | Train Loss: 0.064030 | Val Loss: 0.116582\n",
      "Epoch 138/200 | Train Loss: 0.064083 | Val Loss: 0.117835\n",
      "Epoch 139/200 | Train Loss: 0.063566 | Val Loss: 0.117660\n",
      "Epoch 140/200 | Train Loss: 0.064061 | Val Loss: 0.117689\n",
      "Epoch 141/200 | Train Loss: 0.063830 | Val Loss: 0.114719\n",
      "Epoch 142/200 | Train Loss: 0.063059 | Val Loss: 0.115685\n",
      "Epoch 143/200 | Train Loss: 0.063666 | Val Loss: 0.115391\n",
      "Epoch 144/200 | Train Loss: 0.062882 | Val Loss: 0.117622\n",
      "Epoch 145/200 | Train Loss: 0.063005 | Val Loss: 0.119343\n",
      "Epoch 146/200 | Train Loss: 0.062708 | Val Loss: 0.115646\n",
      "Epoch 147/200 | Train Loss: 0.062759 | Val Loss: 0.118587\n",
      "Epoch 148/200 | Train Loss: 0.062942 | Val Loss: 0.116957\n",
      "Epoch 149/200 | Train Loss: 0.062158 | Val Loss: 0.117465\n",
      "Epoch 150/200 | Train Loss: 0.063301 | Val Loss: 0.116743\n",
      "Epoch 151/200 | Train Loss: 0.061999 | Val Loss: 0.119101\n",
      "Epoch 152/200 | Train Loss: 0.062280 | Val Loss: 0.117131\n",
      "Epoch 153/200 | Train Loss: 0.061276 | Val Loss: 0.117824\n",
      "Epoch 154/200 | Train Loss: 0.062536 | Val Loss: 0.116014\n",
      "Epoch 155/200 | Train Loss: 0.062030 | Val Loss: 0.119915\n",
      "Epoch 156/200 | Train Loss: 0.061311 | Val Loss: 0.119799\n",
      "Epoch 157/200 | Train Loss: 0.061884 | Val Loss: 0.117463\n",
      "Epoch 158/200 | Train Loss: 0.061025 | Val Loss: 0.119095\n",
      "Epoch 159/200 | Train Loss: 0.061451 | Val Loss: 0.119277\n",
      "Epoch 160/200 | Train Loss: 0.060739 | Val Loss: 0.118381\n",
      "Epoch 161/200 | Train Loss: 0.061428 | Val Loss: 0.120672\n",
      "Epoch 162/200 | Train Loss: 0.061258 | Val Loss: 0.119849\n",
      "Epoch 163/200 | Train Loss: 0.061140 | Val Loss: 0.119196\n",
      "Epoch 164/200 | Train Loss: 0.061255 | Val Loss: 0.119089\n",
      "Epoch 165/200 | Train Loss: 0.060153 | Val Loss: 0.120201\n",
      "Epoch 166/200 | Train Loss: 0.060845 | Val Loss: 0.118059\n",
      "Epoch 167/200 | Train Loss: 0.060304 | Val Loss: 0.121478\n",
      "Epoch 168/200 | Train Loss: 0.059647 | Val Loss: 0.120020\n",
      "Epoch 169/200 | Train Loss: 0.059970 | Val Loss: 0.121455\n",
      "Epoch 170/200 | Train Loss: 0.060636 | Val Loss: 0.121479\n",
      "Epoch 171/200 | Train Loss: 0.059359 | Val Loss: 0.120231\n",
      "Epoch 172/200 | Train Loss: 0.058587 | Val Loss: 0.123054\n",
      "Epoch 173/200 | Train Loss: 0.059080 | Val Loss: 0.123373\n",
      "Epoch 174/200 | Train Loss: 0.059683 | Val Loss: 0.123196\n",
      "Epoch 175/200 | Train Loss: 0.059498 | Val Loss: 0.120320\n",
      "Epoch 176/200 | Train Loss: 0.059201 | Val Loss: 0.120659\n",
      "Epoch 177/200 | Train Loss: 0.059272 | Val Loss: 0.122263\n",
      "Epoch 178/200 | Train Loss: 0.059230 | Val Loss: 0.121888\n",
      "Epoch 179/200 | Train Loss: 0.059486 | Val Loss: 0.122186\n",
      "Epoch 180/200 | Train Loss: 0.058880 | Val Loss: 0.119447\n",
      "Epoch 181/200 | Train Loss: 0.058972 | Val Loss: 0.122295\n",
      "Epoch 182/200 | Train Loss: 0.059154 | Val Loss: 0.120254\n",
      "Epoch 183/200 | Train Loss: 0.058422 | Val Loss: 0.120747\n",
      "Epoch 184/200 | Train Loss: 0.058688 | Val Loss: 0.120294\n",
      "Epoch 185/200 | Train Loss: 0.058774 | Val Loss: 0.120663\n",
      "Epoch 186/200 | Train Loss: 0.057646 | Val Loss: 0.125347\n",
      "Epoch 187/200 | Train Loss: 0.058096 | Val Loss: 0.123791\n",
      "Epoch 188/200 | Train Loss: 0.057979 | Val Loss: 0.122243\n",
      "Epoch 189/200 | Train Loss: 0.058711 | Val Loss: 0.119825\n",
      "Epoch 190/200 | Train Loss: 0.058042 | Val Loss: 0.124914\n",
      "Epoch 191/200 | Train Loss: 0.058595 | Val Loss: 0.121213\n",
      "Epoch 192/200 | Train Loss: 0.058354 | Val Loss: 0.122571\n",
      "Epoch 193/200 | Train Loss: 0.057447 | Val Loss: 0.125080\n",
      "Epoch 194/200 | Train Loss: 0.057618 | Val Loss: 0.122495\n",
      "Epoch 195/200 | Train Loss: 0.057108 | Val Loss: 0.119726\n",
      "Epoch 196/200 | Train Loss: 0.057192 | Val Loss: 0.122298\n",
      "Epoch 197/200 | Train Loss: 0.057180 | Val Loss: 0.122495\n",
      "Epoch 198/200 | Train Loss: 0.057020 | Val Loss: 0.124246\n",
      "Epoch 199/200 | Train Loss: 0.057851 | Val Loss: 0.122156\n",
      "Epoch 200/200 | Train Loss: 0.056678 | Val Loss: 0.124655\n",
      "✅ Model Performance on Test Set:\n",
      "  MSE  = 0.124533\n",
      "  RMSE = 0.352892\n",
      "  MAE  = 0.170438\n",
      "  R²   = -4178.946777\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Example dataset (replace with your real data)\n",
    "# -----------------------------\n",
    "class RandomRegressionDataset(Dataset):\n",
    "    def __init__(self, tx, ty):\n",
    "        self.X = torch.tensor(tx)\n",
    "        self.y = torch.tensor(ty)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = RandomRegressionDataset(X_train, y_train)\n",
    "val_dataset   = RandomRegressionDataset(X_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Define regression model\n",
    "# -----------------------------\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self, input_dim=X_train.shape[1], output_dim=y_train.shape[1]):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "model = Regressor()\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Training setup\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Training loop\n",
    "# -----------------------------\n",
    "n_epochs = 200\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(X)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * X.size(0)\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            preds = model(X)\n",
    "            val_loss += criterion(preds, y).item() * X.size(0)\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d}/{n_epochs} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Example prediction\n",
    "# -----------------------------\n",
    "# x_test = torch.randn(1, 1003).to(device)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.tensor(X_test))\n",
    "    mse = mean_squared_error(torch.tensor(y_test), y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(torch.tensor(y_test), y_pred)\n",
    "    r2 = r2_score(torch.tensor(y_test), y_pred)\n",
    "# print(\"Predicted output:\", y_pred.cpu().numpy())\n",
    "    print(f\"✅ Model Performance on Test Set:\")\n",
    "    print(f\"  MSE  = {mse:.6f}\")\n",
    "    print(f\"  RMSE = {rmse:.6f}\")\n",
    "    print(f\"  MAE  = {mae:.6f}\")\n",
    "    print(f\"  R²   = {r2:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{mape_dff:,.2f}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prevent division by zero\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Mean Absolute Percentage Error (MAPE) per feature\n",
    "mape = np.mean(np.abs((y_test - y_pred.cpu().numpy()) / (y_test + epsilon)), axis=0) * 100\n",
    "\n",
    "# Assign column names\n",
    "columns = [\"tar_x\", \"tar_y\", \"tar_z\", \"tar_w\", \"chas_x\", \"chas_y\", \"chas_z\", \"chas_w\"]\n",
    "\n",
    "# Display nicely\n",
    "mape_df = pd.DataFrame({\"Feature\": columns, \"MAPE (%)\": np.round(mape, 2)})\n",
    "print(mape_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a239790c",
   "metadata": {},
   "source": [
    "Feature      MAPE (%)\n",
    "0   tar_x  1.708937e+06\n",
    "1   tar_y  2.686855e+06\n",
    "2   tar_z  2.000483e+06\n",
    "3   tar_w  1.265243e+02\n",
    "4  chas_x  1.295128e+01\n",
    "5  chas_y  1.646947e+01\n",
    "6  chas_z  1.682219e-29\n",
    "7  chas_w  1.824754e-05"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
